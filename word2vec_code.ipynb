{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239afd8e-d4bb-46c1-a268-2f614e2ff391",
   "metadata": {},
   "source": [
    "# Implementation of Word2vector for CBOW training & Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46338082-a6ee-4321-9a0c-8e09c57cfa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600, Loss: 0.514563838759905\n",
      "Words similar to 'success': [('way', 0.48595725752578506), ('best', 0.2952497345919592), ('is', 0.11750311183935276), ('and', 0.0855585172817024), ('to', 0.027338659163779158)]\n",
      "Predicted word given context words ['best', 'way']: to\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Provided text\n",
    "text = ['Best way to success is through hardwork and persistence']\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "tokenized_text = tokenized_text[0]  # Flatten the list of lists\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = set(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word_to_index and index_to_word mappings\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Prepare training data\n",
    "def generate_training_data(tokenized_text, word_to_index, window_size=2):\n",
    "    data = []\n",
    "    for i, word in enumerate(tokenized_text):\n",
    "        target = word_to_index[word]\n",
    "        context = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and j >= 0 and j < len(tokenized_text):\n",
    "                context.append(word_to_index[tokenized_text[j]])\n",
    "        data.append((target, context))\n",
    "    return data\n",
    "\n",
    "training_data = generate_training_data(tokenized_text, word_to_index)\n",
    "\n",
    "class Word2VecCBOW:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "        self.W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def forward_cbow(self, context_indices):\n",
    "        h = np.mean(self.W1[context_indices], axis=0)\n",
    "        u = np.dot(h, self.W2)\n",
    "        y_pred = self.softmax(u)\n",
    "        return y_pred, h\n",
    "    \n",
    "    def backward_cbow(self, error, h, context_indices, learning_rate):\n",
    "        dW2 = np.outer(h, error)\n",
    "        dW1 = np.zeros_like(self.W1)\n",
    "        for context_index in context_indices:\n",
    "            dW1[context_index] += np.dot(self.W2, error)\n",
    "        dW1 /= len(context_indices)\n",
    "        \n",
    "        self.W1[context_indices] -= learning_rate * dW1[context_indices]\n",
    "        self.W2 -= learning_rate * dW2\n",
    "\n",
    "    def train_cbow(self, training_data, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for target, context in training_data:\n",
    "                y_pred, h = self.forward_cbow(context)\n",
    "                error = y_pred.copy()\n",
    "                error[target] -= 1\n",
    "                self.backward_cbow(error, h, context, learning_rate)\n",
    "                loss += -np.log(y_pred[target])\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
    "\n",
    "def get_embedding(word, word_to_index, weight_matrix):\n",
    "    index = word_to_index.get(word)\n",
    "    if index is not None:\n",
    "        return weight_matrix[index]\n",
    "    else:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_similar_words(word, word_to_index, index_to_word, weight_matrix, top_n=5):\n",
    "    target_embedding = get_embedding(word, word_to_index, weight_matrix)\n",
    "    similarities = {}\n",
    "\n",
    "    for idx, embedding in enumerate(weight_matrix):\n",
    "        if index_to_word[idx] != word:\n",
    "            similarity = cosine_similarity(target_embedding, embedding)\n",
    "            similarities[index_to_word[idx]] = similarity\n",
    "\n",
    "    sorted_similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similar_words[:top_n]\n",
    "\n",
    "# Example usage for CBOW\n",
    "embedding_dim = 10\n",
    "word2vec_cbow = Word2VecCBOW(vocab_size, embedding_dim)\n",
    "word2vec_cbow.train_cbow(training_data, epochs=600, learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# Find similar words for 'success'\n",
    "similar_words = find_similar_words('success', word_to_index, index_to_word, word2vec_cbow.W1, top_n=5)\n",
    "print(f\"Words similar to 'success': {similar_words}\")\n",
    "\n",
    "# -------------------------------prediction for multiple words----------------------------------------------------------------------------------------------\n",
    "\n",
    "context_words = ['best','way']\n",
    "context_indices = [word_to_index[word] for word in context_words]\n",
    "y_pred, _ = word2vec_cbow.forward_cbow(context_indices)\n",
    "predicted_word_index = np.argmax(y_pred)\n",
    "predicted_word = index_to_word[predicted_word_index]\n",
    "print(f\"Predicted word given context words {context_words}: {predicted_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abe14b-810b-4640-8ccd-b15facd81a82",
   "metadata": {},
   "source": [
    "# Implementation of Word2vector for SKIP Gram  training & Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9eeda867-6a15-4e14-9f26-644d473bed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.32415973238885\n",
      "Epoch 1001, Loss: 4.154487689590306\n",
      "Words similar to 'success': [('best', 0.664994658832855), ('hardwork', 0.3644387527531994), ('is', 0.16391701713753604), ('to', 0.03118278838720984), ('through', -0.16216656057258505)]\n",
      "Context words for 'success': ['and', 'is', 'through', 'to', 'way']\n"
     ]
    }
   ],
   "source": [
    "# Provided text\n",
    "text = ['Best way to success is through hardwork and persistence']\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "tokenized_text = tokenized_text[0]  # Flatten the list of lists\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = set(tokenized_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word_to_index and index_to_word mappings\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Prepare training data\n",
    "def generate_training_data(tokenized_text, word_to_index, window_size=2):\n",
    "    data = []\n",
    "    for i, word in enumerate(tokenized_text):\n",
    "        target = word_to_index[word]\n",
    "        context = []\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and j >= 0 and j < len(tokenized_text):\n",
    "                context.append(word_to_index[tokenized_text[j]])\n",
    "        data.append((target, context))\n",
    "    return data\n",
    "\n",
    "training_data = generate_training_data(tokenized_text, word_to_index)\n",
    "\n",
    "class Word2VecSkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01  # Smaller initial weights\n",
    "        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01  # Smaller initial weights\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))  # For numerical stability\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def forward_skipgram(self, target_index):\n",
    "        h = self.W1[target_index]\n",
    "        u = np.dot(h, self.W2)\n",
    "        y_pred = self.softmax(u)\n",
    "        return y_pred, h\n",
    "    \n",
    "    def backward_skipgram(self, error, h, target_index, learning_rate):\n",
    "        dW2 = np.outer(h, error)\n",
    "        dW1 = np.dot(self.W2, error)\n",
    "        \n",
    "        self.W1[target_index] -= learning_rate * dW1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "\n",
    "    def train_skipgram(self, training_data, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for target, context in training_data:\n",
    "                y_pred, h = self.forward_skipgram(target)\n",
    "                for context_word in context:\n",
    "                    error = y_pred.copy()\n",
    "                    error[context_word] -= 1\n",
    "                    self.backward_skipgram(error, h, target, learning_rate)\n",
    "                    loss += -np.log(y_pred[context_word] + 1e-9)  # Avoid log(0)\n",
    "            if epoch % 1000 == 0:  # Print loss every 100 epochs\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss / len(training_data)}')  # Average loss\n",
    "\n",
    "def get_embedding(word, word_to_index, weight_matrix):\n",
    "    index = word_to_index.get(word)\n",
    "    if index is not None:\n",
    "        return weight_matrix[index]\n",
    "    else:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_similar_words(word, word_to_index, index_to_word, weight_matrix, top_n=5):\n",
    "    target_embedding = get_embedding(word, word_to_index, weight_matrix)\n",
    "    similarities = {}\n",
    "\n",
    "    for idx, embedding in enumerate(weight_matrix):\n",
    "        if index_to_word[idx] != word:\n",
    "            similarity = cosine_similarity(target_embedding, embedding)\n",
    "            similarities[index_to_word[idx]] = similarity\n",
    "\n",
    "    sorted_similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similar_words[:top_n]\n",
    "\n",
    "# Initialize and train the model\n",
    "embedding_dim = 10\n",
    "word2vec = Word2VecSkipGram(vocab_size, embedding_dim)\n",
    "word2vec.train_skipgram(training_data, epochs=2000, learning_rate=0.01)\n",
    "\n",
    "# Find similar words for 'success'\n",
    "similar_words = find_similar_words('success', word_to_index, index_to_word, word2vec.W1, top_n=5)\n",
    "print(f\"Words similar to 'success': {similar_words}\")\n",
    "\n",
    "# Find context words given 'success'\n",
    "target_word = 'success'\n",
    "context_indices = word_to_index[target_word]\n",
    "y_pred, _ = word2vec_skipgram.forward_skipgram(context_indices)\n",
    "predicted_context_indices = np.argsort(y_pred)[-5:]  # Top 5 predicted context words\n",
    "predicted_context_words = [index_to_word[idx] for idx in predicted_context_indices]\n",
    "print(f\"Context words for '{target_word}': {predicted_context_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595887a6-af93-4d92-aa84-9a06cca2c99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cfeae-9b31-4be1-b095-d1044453528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6522731-1da2-47dd-b4e6-956dac19ce2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
